---
title: Djangoでrobots.txtの設定をする
date: "2018-11-10 00:00"
updated_date: "2021-11-14 22:00"
description: Djangoにrobot.txtを設定する方法を解説
categories: ["Web"]
tags: ["robot.txt"]

---
## 今回やること
クローラーにトップページのみを読み込ませ、バックの管理画面などは読み込ませないようにします。
 また、Djangoにrobots.txtを設置し、Nginxでrobots.txtのエイリアス設定を行います。
 DjagnoとNginxはuWSGIで繋いでいること前提です。

 ## Django側の設定
 読み込ませたいトップページが含まれるアプリケーション内のtemplatesディレクトリに、staticディレクトリを作成してください。
 作成後、staticディレクトリ内にrobots.txtを作成し、以下の設定を書き込みます。

 ```
 User-agent: *
 Allow: /$
 Disallow: /*
 ```

 User-agent: *はすべてクローラーに対して読み込みを許可するという意味です。
 この指定でもいい方はいいのですが、すべてのクローラーを許容することは推奨できないため、信頼できるクローラーのみを指定しておきましょう。
 とりあえず、`Googlebot'は入れておきたいところです。

> クローラーUser-agent一覧
http://www.robotstxt.org/db.html

Allow: /$はhttps://ykonishi.tokyo/は読み込ませ、Disallow: /*はhttps://ykonishi.tokyo/adminなどのスラッシュ以降のURLを読み込ませないという意味です。

## Nginx側の設定
robots.txtのlocationを設定します。
locationの設定の仕方は以下のとおりです。

```
server {

...

location /robots.txt {
alias /home/uichi/project/application/templates/static/robots.txt;
}

...

}
```

aliasにはさきほど作成したrobots.txtのパスを指定してください。

これでuWSGIを再起動すると設定が反映されるはずです。

## 参考
https://syncer.jp/how-to-setting-robots-txt
